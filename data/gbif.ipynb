{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Accessing GBIF data on Azure\n",
    "\n",
    "This notebook provides an example of accessing Global Biodiversity Information Facility (GBIF) occurrence data from blob storage on Azure.  Periodic snapshots of the data are stored in Parquet format.\n",
    "\n",
    "This dataset is stored in the West Europe Azure region, so this notebook will run most efficiently on Azure compute located in the same region.  If you are using this data for environmental science applications, consider applying for an [AI for Earth grant](http://aka.ms/ai4egrants) to support your compute requirements.\n",
    "\n",
    "This dataset is documented at [aka.ms/ai4edata-aster](http://aka.ms/ai4edata-gbif).    \n",
    "\n",
    "### Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618238166691
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from adlfs import AzureBlobFileSystem\n",
    "\n",
    "storage_account_name = 'ai4edataeuwest'\n",
    "sas_token = REDACTED\n",
    "folder_name = 'gbif/occurrence'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Listing the data files\n",
    "\n",
    "GBIF provide an export of occurrence data under the Creative Commons Zero and Creative Commons By-Attribution licenses.  A dataset is uploaded periodically containing georeferenced records available under either license.\n",
    "\n",
    "We can use `adlfs` to view the available data exports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618319312245
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "fs = AzureBlobFileSystem(account_name=storage_account_name, sas_token=sas_token)\n",
    "export_folders = fs.glob(folder_name + '/20*')\n",
    "print('Found {} GBIF data exports'.format(len(export_folders)))\n",
    "for k in range(0,len(export_folders)):\n",
    "    print(export_folders[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We can then list the files within one of these exports, in this case the last (most recent) one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618319312671
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "data_path = export_folders[-1]\n",
    "\n",
    "fs = AzureBlobFileSystem(account_name=storage_account_name, sas_token=sas_token)\n",
    "parquet_files = fs.glob(data_path + '/occurrence.parquet/*')\n",
    "print('Found {} Parquet files in export {}'.format(len(parquet_files), data_path))\n",
    "for k in range(0,5):\n",
    "    print('    ' + parquet_files[k])\n",
    "print('    â€¦')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Opening one data file\n",
    "\n",
    "The whole occurrence dataset has hundreds of millions of records, split across around 100 Parquet files.  We will open just one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618319321710
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(data_path)\n",
    "df = dd.read_parquet('az://' + parquet_files[0], storage_options={'account_name':storage_account_name, 'sas_token':sas_token}).compute()\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Plot data\n",
    "\n",
    "This is a quick plot of latitude and longitude.  We can see the shapes of continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618319325736
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "ax = df.plot.hexbin('decimallongitude', 'decimallatitude', gridsize=(360, 180),\n",
    "                    vmax=100, cmap='Greens', colorbar=False)\n",
    "ax.set_xlim(-180, 180)\n",
    "ax.set_ylim(-90, 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "Another plot shows the month of observation or collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618319326129
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "some_plants = df[(df.kingdom == 'Plantae')]\n",
    "ax = some_plants['month'].plot.hist(x='Month', y='Count', bins=12)\n",
    "\n",
    "ax.set_xlim(1, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Citation\n",
    "\n",
    "It's good practise to cite the data using a DOI (Digital Object Identifier), see GBIF's [Citation Guidelines](https://www.gbif.org/citation-guidelines).\n",
    "\n",
    "GBIF have a service to create a DOI covering a subset of a cloud dataset.  We need to know the DOI of the cloud dataset, and the number of occurrences we used from each contributing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1618319326444
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "print(fs.cat(data_path + '/citation.txt').decode(\"unicode_escape\"))\n",
    "\n",
    "# This is the usage of the month graph, which only used plants:\n",
    "some_plants.groupby(by='datasetkey')['gbifid'].count().reset_index(name='count').sort_values(['count'], ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
