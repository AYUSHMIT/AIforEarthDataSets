{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo notebook for accessing NOAA Global Hydro Estimator data on Azure\n",
    "\n",
    "This notebook provides an example of accessing NOAA Global Hydro Estimator (GHE) data from blob storage on Azure, including (1) finding data files corresponding to a date, (2) retrieving those files from blob storage, (3) opening the downloaded files using the [NetCDF4](https://unidata.github.io/netcdf4-python/netCDF4/index.html) library, and (4) rendering global rainfall on a map.\n",
    "\n",
    "GHE data are stored in the East US data center, so this notebook will run most efficiently on Azure compute located in East US.  We recommend that substantial computation depending on GHE data also be situated in East US.  If you are using GHE data for environmental science applications, consider applying for an [AI for Earth grant](http://aka.ms/ai4egrants) to support your compute requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostly-standard imports\n",
    "import os\n",
    "import gzip\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import shutil\n",
    "import urllib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from scipy.interpolate import interp2d\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Less-common-but-still-pip-installable imports\n",
    "import netCDF4\n",
    "from azure.storage.blob import ContainerClient\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "# pip install progressbar2, not progressbar\n",
    "import progressbar\n",
    "\n",
    "# Storage locations are documented at http://aka.ms/ai4edata-ghe\n",
    "ghe_account_name = 'ghe'\n",
    "ghe_container_name = 'noaa-ghe'\n",
    "ghe_account_url = 'https://' + ghe_account_name + '.blob.core.windows.net'\n",
    "ghe_blob_root = ghe_account_url + '/' + ghe_container_name + '/'\n",
    "\n",
    "# Create a ContainerClient to enumerate blobs\n",
    "ghe_container_client = ContainerClient(account_url=ghe_account_url, \n",
    "                                         container_name=ghe_container_name,\n",
    "                                         credential=None)\n",
    "\n",
    "# The grid spacing for all GHE files is defined in a separate NetCDF file. Uniform\n",
    "# interpolation is close, but it's not perfectly regular.\n",
    "grid_file_url = 'https://ghe.blob.core.windows.net/noaa-ghe/NPR.GEO.GHE.v1.Navigation.netcdf.gz'\n",
    "\n",
    "temp_dir = os.path.join(tempfile.gettempdir(),'ghe')\n",
    "os.makedirs(temp_dir,exist_ok=True)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_url(url, destination_filename=None, progress_updater=None,\n",
    "                 force_download=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Download a URL to a temporary file\n",
    "    \"\"\"\n",
    "    if not verbose:\n",
    "        progress_updater = None\n",
    "        \n",
    "    # This is not intended to guarantee uniqueness, we just know it happens to guarantee\n",
    "    # uniqueness for this application.\n",
    "    if destination_filename is None:\n",
    "        url_as_filename = url.replace('://', '_').replace('/', '_')    \n",
    "        destination_filename = \\\n",
    "            os.path.join(temp_dir,url_as_filename)\n",
    "    if (not force_download) and (os.path.isfile(destination_filename)):\n",
    "        if verbose:\n",
    "            print('Bypassing download of already-downloaded file {}'.format(\n",
    "                os.path.basename(url)))\n",
    "        return destination_filename\n",
    "    if verbose:\n",
    "        print('Downloading file {} to {}'.format(os.path.basename(url),\n",
    "                                                 destination_filename),end='')\n",
    "    urllib.request.urlretrieve(url, destination_filename, progress_updater)  \n",
    "    assert(os.path.isfile(destination_filename))\n",
    "    nBytes = os.path.getsize(destination_filename)\n",
    "    if verbose:\n",
    "        print('...done, {} bytes.'.format(nBytes))\n",
    "    return destination_filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the grid spacing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file is ~150MB, so best to cache this\n",
    "grid_filename_gz = download_url(grid_file_url,verbose=True)\n",
    "with gzip.open(grid_filename_gz) as gz:\n",
    "        grid_dataset = netCDF4.Dataset('dummy', mode='r', memory=gz.read())\n",
    "        print(grid_dataset.variables)\n",
    "        lat_grid_raw = grid_dataset['latitude']\n",
    "        lon_grid_raw = grid_dataset['longitude']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data are stored as product/year/month/day/filename\n",
    "product = 'rain_rate'\n",
    "\n",
    "# Grab data from April 9, 2020\n",
    "syear = '2020'; smonth = '04'; sday = '09'\n",
    "\n",
    "# Filenames look like:\n",
    "#\n",
    "# NPR.GEO.GHE.v1.S202001170000.nc.gz\n",
    "#\n",
    "# ...where the last four digits represent time, n increments of 15 minutes from 0000\n",
    "\n",
    "# We can either sum over a whole day, or take a single 15-minute window\n",
    "single_time_point = False\n",
    "\n",
    "if single_time_point:\n",
    "    \n",
    "    # Pick an arbitrary time of day to plot\n",
    "    stime = '0200'\n",
    "    \n",
    "    filename = 'NPR.GEO.GHE.v1.S' + syear + smonth + sday + stime + '.nc.gz'\n",
    "    blob_urls = [ghe_blob_root + product + '/' + syear + '/' + smonth + '/' + sday + '/' \\\n",
    "                 + filename]\n",
    "    \n",
    "else:\n",
    "    \n",
    "    prefix = product + '/' + syear + '/' + smonth + '/' + sday\n",
    "    print('Finding blobs matching prefix: {}'.format(prefix))\n",
    "    generator = ghe_container_client.list_blobs(name_starts_with=prefix)\n",
    "    blob_urls = []\n",
    "    for blob in generator:\n",
    "        blob_urls.append(ghe_blob_root + blob.name)\n",
    "    print('Found {} matching scans'.format(len(blob_urls)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read and sum the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rainfall = None\n",
    "variable_description = None\n",
    "\n",
    "n_valid = np.zeros(lat_grid_raw.shape)\n",
    "rainfall = np.zeros(lat_grid_raw.shape)\n",
    "\n",
    "for i_blob,blob_url in tqdm(enumerate(blob_urls),total=len(blob_urls)):\n",
    "    \n",
    "    # Typical files are ~3MB compressed\n",
    "    filename = download_url(blob_url,verbose=False)\n",
    "\n",
    "    # NetCDF4 can read directly from gzip without unzipping the file to disk\n",
    "    with gzip.open(filename) as gz:\n",
    "        dataset = netCDF4.Dataset('dummy', mode='r', memory=gz.read())\n",
    "\n",
    "    rainfall_sample = dataset['rain'][:]\n",
    "    \n",
    "    # There are fill values in here where data isn't available.  If we were really trying to\n",
    "    # produce global rainfall estimates over a fixed time period, we would think carefully\n",
    "    # about what we want to do with those invalid values, e.g. averaging over all the *valid*\n",
    "    # values at each grid cell, instead of summing.\n",
    "    rainfall_sample[rainfall_sample < 0] = 0\n",
    "    \n",
    "    variable_description = str(dataset.variables)        \n",
    "    rain_units = dataset['rain'].units\n",
    "    rainfall = rainfall + rainfall_sample\n",
    "        \n",
    "    dataset.close()\n",
    "\n",
    "min_rf = np.min(rainfall)\n",
    "max_rf = np.max(rainfall)\n",
    "\n",
    "print('Ranfall ranges from {}{} to {}{}'.format(min_rf,rain_units,max_rf,rain_units))\n",
    "\n",
    "# Make a 'backup' so we can tinker, as one does in notebooks\n",
    "rainfall_raw = rainfall.copy();\n",
    "\n",
    "# Take a look at what's in each NetCDF file\n",
    "print(variable_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare indices,  downsample for faster plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = np.shape(rainfall_raw)\n",
    "nlat = image_size[0]; nlon = image_size[1]\n",
    "\n",
    "assert(np.shape(rainfall_raw)==np.shape(lat_grid_raw))\n",
    "assert(np.shape(rainfall_raw)==np.shape(lon_grid_raw))\n",
    "\n",
    "# Downsample by decimation\n",
    "ds_factor = 10\n",
    "\n",
    "lon_grid = lon_grid_raw[::ds_factor,::ds_factor,]\n",
    "lat_grid = lat_grid_raw[::ds_factor,::ds_factor,]\n",
    "rainfall = rainfall_raw[::ds_factor,::ds_factor,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "# Prepare a matplotlib Basemap so we can render coastlines and borders\n",
    "m = Basemap(projection='merc',\n",
    "  llcrnrlon=np.nanmin(lon_grid),urcrnrlon=np.nanmax(lon_grid),\n",
    "  llcrnrlat=np.nanmin(lat_grid),urcrnrlat=np.nanmax(lat_grid),\n",
    "  resolution='c')\n",
    "\n",
    "# Convert lat/lon to a 2D grid\n",
    "# lon_grid,lat_grid = np.meshgrid(lon,lat)\n",
    "x,y = m(lon_grid,lat_grid)\n",
    "\n",
    "# Clip our plot values to an upper threshold, and leave anything\n",
    "# below the lower threshold as white (i.e., unplotted)\n",
    "n_files = len(blob_urls)\n",
    "upper_plot_threshold = n_files*10\n",
    "lower_plot_threshold = n_files*0.01\n",
    "\n",
    "Z = rainfall.copy()\n",
    "Z[Z > upper_plot_threshold] = upper_plot_threshold\n",
    "Z[Z < lower_plot_threshold] = np.nan\n",
    "Z = np.ma.masked_where(np.isnan(Z),Z)\n",
    "\n",
    "# Choose normalization and color mapping\n",
    "norm = mpl.colors.LogNorm(vmin=Z.min(), vmax=Z.max(), clip=True)\n",
    "cmap = plt.cm.Blues\n",
    "\n",
    "# Plot as a color mesh\n",
    "cs = m.pcolormesh(x,y,Z,norm=norm,cmap=cmap)\n",
    "\n",
    "# Draw extra stuff to make our plot look fancier... sweeping clouds on a plain background\n",
    "# are great, but sweeping clouds on contentinal outlines are *very* satisfying.\n",
    "m.drawcoastlines()\n",
    "m.drawmapboundary()\n",
    "m.drawparallels(np.arange(-90.,120.,30.),labels=[1,0,0,0])\n",
    "m.drawmeridians(np.arange(-180.,180.,60.),labels=[0,0,0,1])\n",
    "m.colorbar(cs)\n",
    "\n",
    "plt.title('Global rainfall ({})'.format(rain_units))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
